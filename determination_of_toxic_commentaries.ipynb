{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span><ul class=\"toc-item\"><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Logistic Regression</a></span></li><li><span><a href=\"#LinearSVC\" data-toc-modified-id=\"LinearSVC-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>LinearSVC</a></span></li></ul></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as nltk_stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from tqdm import tqdm\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159292, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Токсичных комментариев - 16186, не токсичных - 143106.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Лемматизируем данные</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk_stopwords.words('english'))\n",
    "@lru_cache(maxsize=128)\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Расширенное преобразование POS-тегов с учетом большего количества случаев\"\"\"\n",
    "    tag_mapping = {\n",
    "        'J': wordnet.ADJ,    # Прилагательное\n",
    "        'V': wordnet.VERB,   # Глагол\n",
    "        'N': wordnet.NOUN,   # Существительное\n",
    "        'R': wordnet.ADV,    # Наречие\n",
    "        'S': wordnet.ADJ_SAT # Прилагательное (сателлит)\n",
    "    }\n",
    "    \n",
    "    # Берем первую букву тега\n",
    "    first_char = treebank_tag[0].upper()\n",
    "    \n",
    "    # Возвращаем соответствующий тег WordNet или NOUN по умолчанию\n",
    "    return tag_mapping.get(first_char, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_with_pos(text, remove_stopwords=True, keep_pos_tags=False):\n",
    "    # Токенизация и получение POS-тегов\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Лемматизация с учетом POS\n",
    "    processed = []\n",
    "    for word, tag in pos_tags:\n",
    "        \n",
    "        if remove_stopwords and word.lower() in stop_words:\n",
    "            continue\n",
    "        \n",
    "        wordnet_pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos=wordnet_pos)\n",
    "        \n",
    "        # Обработка особых случаев\n",
    "        if lemma.lower() == \"'s\":\n",
    "            lemma = 'is'\n",
    "        elif lemma.lower() == \"'re\":\n",
    "            lemma = 'are'\n",
    "        elif lemma.lower() == \"'m\":\n",
    "            lemma = 'am'\n",
    "            \n",
    "        processed.append((lemma, tag) if keep_pos_tags else lemma)\n",
    "    \n",
    "    return processed if keep_pos_tags else ' '.join(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                        text  \\\n",
      "0   1  The striped bats are hanging on their feet   \n",
      "1   2           She's running quickly in the park   \n",
      "2   3    I'm feeling happier today than yesterday   \n",
      "\n",
      "                 lemmatized_text  \n",
      "0          striped bat hang foot  \n",
      "1            is run quickly park  \n",
      "2  am feel happy today yesterday  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Предположим, у нас есть DataFrame с текстовым столбцом 'text'\n",
    "tqdm.pandas() \n",
    "df = pd.DataFrame({\n",
    "    'id': [1, 2, 3],\n",
    "    'text': [\n",
    "        \"The striped bats are hanging on their feet\",\n",
    "        \"She's running quickly in the park\",\n",
    "        \"I'm feeling happier today than yesterday\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Применяем лемматизацию к столбцу 'text'\n",
    "df['lemmatized_text'] = df['text'].progress_apply(lemmatize_with_pos)\n",
    "\n",
    "# Результат\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z ]', ' ', text)\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [10:15<00:00, 258.84it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_lemm = data['text'].progress_apply(lemmatize_with_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Explanation edits make username Hardcore Metal...\n",
       "1         D'aww ! match background colour am seemingly s...\n",
       "2         Hey man , am really try edit war . is guy cons...\n",
       "3         `` ca n't make real suggestion improvement - w...\n",
       "4                  , sir , hero . chance remember page is ?\n",
       "                                ...                        \n",
       "159287    `` : : : : : second time ask , view completely...\n",
       "159288    ashamed horrible thing put talk page . 128.61....\n",
       "159289    Spitzer Umm , theres actual article prostituti...\n",
       "159290    look like actually put speedy first version de...\n",
       "159291    `` ... really n't think understand . come idea...\n",
       "Name: text, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_lemm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Разобьём данные на выборки</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(corpus_lemm, data['toxic'], \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочной корпуса: 127433\n",
      "Размер тестового корпуса: 31859\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочной корпуса: {len(X_train)}\")\n",
    "print(f\"Размер тестового корпуса: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Проведём векторизацию корпусов с помощью TfidfVectorizer и удалим стоп-слова.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vec = TfidfVectorizer(ngram_range=(1,1), stop_words=stop_words,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vec = tf_idf_vec.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vec = tf_idf_vec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер тренировочного датасета: (127433, 41337)\n",
      "Размер тестового датасета: (31859, 41337)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Размер тренировочного датасета: {X_train_vec.shape}\")\n",
    "print(f\"Размер тестового датасета: {X_test_vec.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Сначала обучим логистическую регрессию. Будем подбирать гиперпараметр регуляризации C с помощью RandomizedSearchCV.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/model_selection/_search.py:285: UserWarning: The total space of parameters 5 is smaller than n_iter=10. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ..............................C=10.0, max_iter=1000; total time= 1.3min\n",
      "[CV] END ..............................C=10.0, max_iter=1000; total time=  51.8s\n",
      "[CV] END ..............................C=10.0, max_iter=1000; total time= 1.5min\n",
      "[CV] END ..............................C=10.0, max_iter=1000; total time= 1.2min\n",
      "[CV] END ..............................C=10.0, max_iter=1000; total time= 1.5min\n",
      "[CV] END ..............................C=12.5, max_iter=1000; total time= 1.1min\n",
      "[CV] END ..............................C=12.5, max_iter=1000; total time= 1.4min\n",
      "[CV] END ..............................C=12.5, max_iter=1000; total time= 1.4min\n",
      "[CV] END ..............................C=12.5, max_iter=1000; total time= 1.3min\n",
      "[CV] END ..............................C=12.5, max_iter=1000; total time= 1.2min\n",
      "[CV] END ..............................C=15.0, max_iter=1000; total time= 1.5min\n",
      "[CV] END ..............................C=15.0, max_iter=1000; total time= 1.3min\n",
      "[CV] END ..............................C=15.0, max_iter=1000; total time= 1.4min\n",
      "[CV] END ..............................C=15.0, max_iter=1000; total time= 1.5min\n",
      "[CV] END ..............................C=15.0, max_iter=1000; total time= 1.5min\n",
      "[CV] END ..............................C=17.5, max_iter=1000; total time= 1.7min\n",
      "[CV] END ..............................C=17.5, max_iter=1000; total time= 1.4min\n",
      "[CV] END ..............................C=17.5, max_iter=1000; total time= 1.4min\n",
      "[CV] END ..............................C=17.5, max_iter=1000; total time= 1.6min\n",
      "[CV] END ..............................C=17.5, max_iter=1000; total time= 1.7min\n",
      "[CV] END ..............................C=20.0, max_iter=1000; total time= 1.6min\n",
      "[CV] END ..............................C=20.0, max_iter=1000; total time= 1.5min\n",
      "[CV] END ..............................C=20.0, max_iter=1000; total time= 1.6min\n",
      "[CV] END ..............................C=20.0, max_iter=1000; total time= 1.7min\n",
      "[CV] END ..............................C=20.0, max_iter=1000; total time= 1.5min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=LogisticRegression(random_state=42),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'C': array([10. , 12.5, 15. , 17.5, 20. ]),\n",
       "                                        'max_iter': [1000]},\n",
       "                   scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(10, 20, num = 5, endpoint = True),\n",
    "             'max_iter': [1000]}\n",
    "lrm = LogisticRegression(random_state=42)\n",
    "clf = RandomizedSearchCV(lrm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)\n",
    "clf.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший показатель f1 на кросс-валидации : 0.771\n",
      "Параметр регуляризации для лучшей модели: {'max_iter': 1000, 'C': 17.5}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Лучший показатель f1 на кросс-валидации : {clf.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>F1 у нашей логистической регрессии 0.773, всё отлично.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 7 candidates, totalling 35 fits\n",
      "[CV] END ..............................................C=1.0; total time=   0.7s\n",
      "[CV] END ..............................................C=1.0; total time=   0.8s\n",
      "[CV] END ..............................................C=1.0; total time=   0.9s\n",
      "[CV] END ..............................................C=1.0; total time=   0.9s\n",
      "[CV] END ..............................................C=1.0; total time=   0.8s\n",
      "[CV] END ..............................................C=6.0; total time=   2.2s\n",
      "[CV] END ..............................................C=6.0; total time=   2.2s\n",
      "[CV] END ..............................................C=6.0; total time=   2.3s\n",
      "[CV] END ..............................................C=6.0; total time=   2.2s\n",
      "[CV] END ..............................................C=6.0; total time=   2.3s\n",
      "[CV] END .............................................C=11.0; total time=   3.2s\n",
      "[CV] END .............................................C=11.0; total time=   3.3s\n",
      "[CV] END .............................................C=11.0; total time=   4.6s\n",
      "[CV] END .............................................C=11.0; total time=   3.5s\n",
      "[CV] END .............................................C=11.0; total time=   3.5s\n",
      "[CV] END .............................................C=16.0; total time=   5.7s\n",
      "[CV] END .............................................C=16.0; total time=   5.9s\n",
      "[CV] END .............................................C=16.0; total time=   6.0s\n",
      "[CV] END .............................................C=16.0; total time=   4.8s\n",
      "[CV] END .............................................C=16.0; total time=   5.7s\n",
      "[CV] END .............................................C=21.0; total time=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=21.0; total time=   6.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=21.0; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=21.0; total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=21.0; total time=   6.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=26.0; total time=   6.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=26.0; total time=   6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=26.0; total time=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=26.0; total time=   6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=26.0; total time=   7.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=31.0; total time=   6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=31.0; total time=   6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=31.0; total time=   7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=31.0; total time=   6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/svm/_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END .............................................C=31.0; total time=   6.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LinearSVC(), n_jobs=-1,\n",
       "             param_grid={'C': array([ 1.,  6., 11., 16., 21., 26., 31.])},\n",
       "             scoring='f1', verbose=2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'C': np.linspace(1, 31, num = 7, endpoint = True)}\n",
    "lsvcm = LinearSVC(max_iter = 1000)\n",
    "clf_lsvc = GridSearchCV(lsvcm, parameters,\n",
    "                  cv=5,\n",
    "                  scoring='f1',\n",
    "                  n_jobs=-1,\n",
    "                  verbose=2)\n",
    "clf_lsvc.fit(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучший показатель f1 на кросс-валидации : 0.775\n",
      "Параметр регуляризации для лучшей модели: {'C': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Лучший показатель f1 на кросс-валидации : {clf_lsvc.best_score_:.3f}\")\n",
    "print(f\"Параметр регуляризации для лучшей модели: {clf_lsvc.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Метрика f1 также больше 0.75, а именно она составляет 0.776.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Судя по метрике f1, выигрывает LinearSVC, выберем именно её для итогового тестирования.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Показатель f1 на тестовой выборке: 0.778\n"
     ]
    }
   ],
   "source": [
    "predictions = clf_lsvc.best_estimator_.predict(X_test_vec)\n",
    "f1 = f1_score(y_test, predictions)\n",
    "print(f\"Показатель f1 на тестовой выборке: {f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Успешно загрузили и обработали данные (лемматизация, указание стоп-слов и т.д.)\n",
    "* Лемматизация была проведена с помощью WordNetLemmatizer\n",
    "* Привели корпус к вектору с помощью TfidfVectorizer\n",
    "\n",
    "<b>Затем обучили две модели: LogisticRegression и LinearSVC</b>\n",
    "* У LogisticRegression на тренировочной выборке f1 составляет 0.773\n",
    "* У LinearSVC на тренировочной выборке f1 составляет 0.776\n",
    "\n",
    "<b>Подбор гиперпараметров осуществлялся с помощью RandomizedSearchCV и GridSearchCV.</b>\n",
    "\n",
    "\n",
    "<b>По результатам f1, выбрали LinearSVC, у которой на тестовой выборке f1 составил 0.779.</b>"
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2887,
    "start_time": "2025-05-19T08:41:39.220Z"
   },
   {
    "duration": 202,
    "start_time": "2025-05-19T08:42:28.716Z"
   },
   {
    "duration": 18,
    "start_time": "2025-05-19T08:42:40.544Z"
   },
   {
    "duration": 53,
    "start_time": "2025-05-19T08:42:49.616Z"
   },
   {
    "duration": 84,
    "start_time": "2025-05-19T08:42:57.222Z"
   },
   {
    "duration": 913,
    "start_time": "2025-05-19T08:47:09.651Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T08:47:29.089Z"
   },
   {
    "duration": 923,
    "start_time": "2025-05-19T08:47:31.896Z"
   },
   {
    "duration": 13,
    "start_time": "2025-05-19T08:47:34.194Z"
   },
   {
    "duration": 11,
    "start_time": "2025-05-19T08:48:43.266Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T08:48:47.138Z"
   },
   {
    "duration": 6,
    "start_time": "2025-05-19T08:49:10.736Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T08:51:30.371Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T08:51:31.093Z"
   },
   {
    "duration": 3128,
    "start_time": "2025-05-19T08:51:31.663Z"
   },
   {
    "duration": 87230,
    "start_time": "2025-05-19T08:51:37.197Z"
   },
   {
    "duration": 39,
    "start_time": "2025-05-19T08:54:27.738Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T08:54:47.889Z"
   },
   {
    "duration": 60,
    "start_time": "2025-05-19T08:57:03.069Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T08:58:03.738Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T09:08:09.039Z"
   },
   {
    "duration": 5669,
    "start_time": "2025-05-19T09:09:40.259Z"
   },
   {
    "duration": 17,
    "start_time": "2025-05-19T09:11:44.686Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T09:11:49.904Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T09:12:05.245Z"
   },
   {
    "duration": 4870,
    "start_time": "2025-05-19T09:12:19.013Z"
   },
   {
    "duration": 1464,
    "start_time": "2025-05-19T09:13:04.722Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T09:13:22.712Z"
   },
   {
    "duration": 7,
    "start_time": "2025-05-19T09:25:39.701Z"
   },
   {
    "duration": 6,
    "start_time": "2025-05-19T09:25:48.172Z"
   },
   {
    "duration": 6,
    "start_time": "2025-05-19T09:26:15.788Z"
   },
   {
    "duration": 6,
    "start_time": "2025-05-19T09:26:23.191Z"
   },
   {
    "duration": 7,
    "start_time": "2025-05-19T09:26:33.710Z"
   },
   {
    "duration": 5,
    "start_time": "2025-05-19T09:26:44.593Z"
   },
   {
    "duration": 592031,
    "start_time": "2025-05-19T09:51:16.747Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:01:08.780Z"
   },
   {
    "duration": 246,
    "start_time": "2025-05-19T10:01:12.440Z"
   },
   {
    "duration": 2743,
    "start_time": "2025-05-19T10:09:15.892Z"
   },
   {
    "duration": 910,
    "start_time": "2025-05-19T10:09:18.637Z"
   },
   {
    "duration": 14,
    "start_time": "2025-05-19T10:09:19.548Z"
   },
   {
    "duration": 13,
    "start_time": "2025-05-19T10:09:19.564Z"
   },
   {
    "duration": 8,
    "start_time": "2025-05-19T10:09:19.579Z"
   },
   {
    "duration": 15,
    "start_time": "2025-05-19T10:09:19.589Z"
   },
   {
    "duration": 42,
    "start_time": "2025-05-19T10:09:19.606Z"
   },
   {
    "duration": 2434,
    "start_time": "2025-05-19T10:09:19.649Z"
   },
   {
    "duration": 90252,
    "start_time": "2025-05-19T10:09:22.085Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T10:10:52.338Z"
   },
   {
    "duration": 225,
    "start_time": "2025-05-19T10:10:52.344Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.571Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.572Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.573Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.574Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.575Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.576Z"
   },
   {
    "duration": 0,
    "start_time": "2025-05-19T10:10:52.577Z"
   },
   {
    "duration": 62,
    "start_time": "2025-05-19T10:32:12.814Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T10:32:13.710Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T10:32:14.039Z"
   },
   {
    "duration": 5578,
    "start_time": "2025-05-19T10:32:14.190Z"
   },
   {
    "duration": 1380,
    "start_time": "2025-05-19T10:32:19.770Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T10:32:21.153Z"
   },
   {
    "duration": 18,
    "start_time": "2025-05-19T10:32:21.158Z"
   },
   {
    "duration": 2125499,
    "start_time": "2025-05-19T10:32:21.178Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T11:18:13.790Z"
   },
   {
    "duration": 7,
    "start_time": "2025-05-19T13:07:35.866Z"
   },
   {
    "duration": 13,
    "start_time": "2025-05-19T13:10:50.921Z"
   },
   {
    "duration": 161425,
    "start_time": "2025-05-19T13:11:26.106Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T13:14:15.846Z"
   },
   {
    "duration": 5,
    "start_time": "2025-05-19T13:19:43.273Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T13:19:48.119Z"
   },
   {
    "duration": 8,
    "start_time": "2025-05-19T13:19:51.997Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T13:22:29.411Z"
   },
   {
    "duration": 23,
    "start_time": "2025-05-19T13:24:26.078Z"
   },
   {
    "duration": 16,
    "start_time": "2025-05-19T13:24:31.742Z"
   },
   {
    "duration": 2771,
    "start_time": "2025-05-19T21:30:49.458Z"
   },
   {
    "duration": 886,
    "start_time": "2025-05-19T21:30:53.681Z"
   },
   {
    "duration": 14,
    "start_time": "2025-05-19T21:30:54.569Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T21:30:55.027Z"
   },
   {
    "duration": 8,
    "start_time": "2025-05-19T21:30:55.214Z"
   },
   {
    "duration": 175,
    "start_time": "2025-05-19T21:30:59.918Z"
   },
   {
    "duration": 516,
    "start_time": "2025-05-19T21:31:22.931Z"
   },
   {
    "duration": 1427,
    "start_time": "2025-05-19T21:31:26.768Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T21:31:55.302Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T21:31:57.570Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T21:32:58.907Z"
   },
   {
    "duration": 5,
    "start_time": "2025-05-19T21:38:35.410Z"
   },
   {
    "duration": 16,
    "start_time": "2025-05-19T21:38:48.564Z"
   },
   {
    "duration": 13,
    "start_time": "2025-05-19T21:39:10.592Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T21:39:26.245Z"
   },
   {
    "duration": 287,
    "start_time": "2025-05-19T21:39:33.081Z"
   },
   {
    "duration": 8,
    "start_time": "2025-05-19T21:40:45.738Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T21:40:49.653Z"
   },
   {
    "duration": 11,
    "start_time": "2025-05-19T21:41:19.873Z"
   },
   {
    "duration": 11,
    "start_time": "2025-05-19T21:41:22.022Z"
   },
   {
    "duration": 80202,
    "start_time": "2025-05-19T21:41:51.630Z"
   },
   {
    "duration": 46,
    "start_time": "2025-05-19T21:43:14.168Z"
   },
   {
    "duration": 8,
    "start_time": "2025-05-19T21:44:23.974Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T21:44:26.041Z"
   },
   {
    "duration": 578553,
    "start_time": "2025-05-19T21:44:31.253Z"
   },
   {
    "duration": 6,
    "start_time": "2025-05-19T21:54:43.299Z"
   },
   {
    "duration": 33,
    "start_time": "2025-05-19T21:55:07.643Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-19T21:55:08.647Z"
   },
   {
    "duration": 11,
    "start_time": "2025-05-19T21:55:09.660Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T21:55:09.836Z"
   },
   {
    "duration": 12,
    "start_time": "2025-05-19T21:55:10.013Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T21:55:18.851Z"
   },
   {
    "duration": 5271,
    "start_time": "2025-05-19T21:55:19.721Z"
   },
   {
    "duration": 1303,
    "start_time": "2025-05-19T21:55:24.995Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-19T21:55:35.931Z"
   },
   {
    "duration": 2354991,
    "start_time": "2025-05-19T21:55:39.553Z"
   },
   {
    "duration": 52,
    "start_time": "2025-05-20T06:19:54.278Z"
   },
   {
    "duration": 3409,
    "start_time": "2025-05-20T06:20:00.173Z"
   },
   {
    "duration": 963,
    "start_time": "2025-05-20T06:20:03.584Z"
   },
   {
    "duration": 15,
    "start_time": "2025-05-20T06:20:04.549Z"
   },
   {
    "duration": 20,
    "start_time": "2025-05-20T06:20:04.566Z"
   },
   {
    "duration": 14,
    "start_time": "2025-05-20T06:20:04.588Z"
   },
   {
    "duration": 57,
    "start_time": "2025-05-20T06:20:04.604Z"
   },
   {
    "duration": 25,
    "start_time": "2025-05-20T06:20:04.663Z"
   },
   {
    "duration": 1490,
    "start_time": "2025-05-20T06:20:04.690Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-20T06:20:06.182Z"
   },
   {
    "duration": 117,
    "start_time": "2025-05-20T06:20:06.186Z"
   },
   {
    "duration": 615430,
    "start_time": "2025-05-20T06:20:06.306Z"
   },
   {
    "duration": 6,
    "start_time": "2025-05-20T06:30:21.738Z"
   },
   {
    "duration": 49,
    "start_time": "2025-05-20T06:30:21.745Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-20T06:30:21.797Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-20T06:30:21.801Z"
   },
   {
    "duration": 5552,
    "start_time": "2025-05-20T06:30:21.807Z"
   },
   {
    "duration": 1382,
    "start_time": "2025-05-20T06:30:27.361Z"
   },
   {
    "duration": 4,
    "start_time": "2025-05-20T06:30:28.744Z"
   },
   {
    "duration": 2252528,
    "start_time": "2025-05-20T06:30:28.749Z"
   },
   {
    "duration": 5,
    "start_time": "2025-05-20T07:08:01.279Z"
   },
   {
    "duration": 161217,
    "start_time": "2025-05-20T07:08:01.287Z"
   },
   {
    "duration": 3,
    "start_time": "2025-05-20T07:10:42.505Z"
   },
   {
    "duration": 31,
    "start_time": "2025-05-20T07:10:42.510Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
